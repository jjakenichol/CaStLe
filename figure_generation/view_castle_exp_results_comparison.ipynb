{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from graph_metrics import F1_score, matthews_correlation_coefficient, false_discovery_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRIDS = [4, 5, 6, 7, 8, 9, 10]\n",
    "Ts = [10, 150]\n",
    "SIGMAS = [1.0]\n",
    "DENSITIES = [0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "DENSITY_LABELS = [\"$\\\\frac{1}{9}$\", \"$\\\\frac{2}{9}$\", \"$\\\\frac{3}{9}$\", \"$\\\\frac{4}{9}$\", \"$\\\\frac{5}{9}$\", \"$\\\\frac{6}{9}$\", \"$\\\\frac{7}{9}$\", \"$\\\\frac{8}{9}$\", \"$\\\\frac{9}{9}$\"]\n",
    "MIN_VALUE = 0.1\n",
    "MODE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/path/to/benchmark/data/castle/\"\n",
    "VAR_results_root = \"/path/to/benchmark/data/castle/VAR_graphs_results/r_\"\n",
    "VAR_csv_filename = \"/data/castle/VAR_graphs_results.csv\"\n",
    "PC_results_root = \"/path/to/benchmark/data/castle/PC_results/r_\"\n",
    "PC_csv_filename = \"/data/castle/PC_results.csv\"\n",
    "PCMCI_results_root = \"/path/to/benchmark/data/castle/PCMCI_results/r_\"\n",
    "PCMCI_csv_filename = \"/data/castle/PCMCI_results.csv\"\n",
    "DYNOTEARS_results_root = \"/path/to/benchmark/data/castle/DYNOT_results/r_\"\n",
    "DYNOTEARS_csv_filename = \"/data/castle/DYNOT_results.csv\"\n",
    "castlePC_results_root = \"/path/to/benchmark/data/castle/PC_CaStLe_results/r_\"\n",
    "castlePC_csv_filename = \"/data/castle/PC_CaStLe_results.csv\"\n",
    "castlePCMCI_results_root = \"/path/to/benchmark/data/castle/PCMCI_CaStLe_results/r_\"\n",
    "castlePCMCI_csv_filename = \"/data/castle/PCMCI_CaStLe_results.csv\"\n",
    "castleVAR_results_root = \"/path/to/benchmark/data/castle/VAR_CaStLe_results/r_\"\n",
    "castleVAR_csv_filename = \"/data/castle/VAR_CaStLe_results.csv\"\n",
    "castleDYNOTEARS_results_root = \"/path/to/benchmark/data/castle/DYNOT_CaStLe_results/r_\"\n",
    "castleDYNOTEARS_csv_filename = \"/data/castle/DYNOT_CaStLe_results.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "method_meta_data = {\n",
    "    \"VAR\": [VAR_results_root, VAR_csv_filename],\n",
    "    \"CaStLe-VAR\": [castleVAR_results_root, castleVAR_csv_filename],\n",
    "    \"PC\": [PC_results_root, PC_csv_filename],\n",
    "    \"PCMCI\": [PCMCI_results_root, PCMCI_csv_filename],\n",
    "    \"CaStLe-PC\": [castlePC_results_root, castlePC_csv_filename],\n",
    "    \"CaStLe-PCMCI\": [castlePCMCI_results_root, castlePCMCI_csv_filename],\n",
    "    \"DYNOTEARS\": [DYNOTEARS_results_root, DYNOTEARS_csv_filename],\n",
    "    \"CaStLe-DYNOTEARS\": [castleDYNOTEARS_results_root, castleDYNOTEARS_csv_filename],\n",
    "}\n",
    "\n",
    "idx = 0\n",
    "for method in method_meta_data:\n",
    "    print(method)\n",
    "    results_root = method_meta_data[method][0]\n",
    "    csv_filename = method_meta_data[method][1]\n",
    "    csv_loaded = False\n",
    "    try:\n",
    "        with open(str(Path.home()) + csv_filename, \"rb\") as f:\n",
    "            # raise NameError\n",
    "            results_df = pd.read_csv(f, index_col=0)\n",
    "        csv_loaded = True\n",
    "        method_meta_data[method].append(results_df)\n",
    "    except Exception as e: \n",
    "        print(\"Exception encountered in method {}: {}\".format(method, e))\n",
    "        SAVE_DATA = True\n",
    "        LOG_DATA_COMPLETED = True\n",
    "        LOG_RESULTS_COMPLETED = True\n",
    "        data_incomplete = []\n",
    "        data_complete = []\n",
    "        r_incomplete = []\n",
    "        r_complete = []\n",
    "        corrupted = []\n",
    "\n",
    "        if LOG_DATA_COMPLETED and idx > 0:\n",
    "            LOG_DATA_COMPLETED = False\n",
    "\n",
    "        results_dict_list = []\n",
    "        for grid_size in GRIDS:\n",
    "            for T in Ts:\n",
    "                for sigma in SIGMAS:\n",
    "                    for density in DENSITIES:\n",
    "                        data_name = (\n",
    "                                str(grid_size) \n",
    "                                + \"x\"\n",
    "                                + str(grid_size)\n",
    "                                + \"_\"\n",
    "                                + str(T)\n",
    "                                + \"T_\"\n",
    "                                + str(sigma)\n",
    "                                + \"sigma_\"\n",
    "                                + str(density)\n",
    "                                + \"density_\"\n",
    "                                + str(MIN_VALUE)\n",
    "                                + \"minval_wMode-\"\n",
    "                                + str(MODE)\n",
    "                                + \"_*\"\n",
    "                        )\n",
    "                        DATA_PATH = data_root + data_name\n",
    "                        RESULTS_PATH = results_root + data_name\n",
    "\n",
    "                        for i, filename in enumerate(glob.glob(RESULTS_PATH)):\n",
    "                            replicate_num = filename.split(\"_\")[-1].split(\".\")[0]\n",
    "                            with open(filename, \"rb\") as npyfile:\n",
    "                                try:\n",
    "                                    coefficients, reconstructed_graph, true_graph_metrics, recon_graph_metrics, equivalence_matrix, f1_result = np.load(npyfile, allow_pickle=True)\n",
    "                                except pickle.UnpicklingError:\n",
    "                                    print(\"Corrupted File Found!\")\n",
    "                                    corrupted.append(npyfile)\n",
    "                                    continue\n",
    "                                results_dict_list.append(\n",
    "                                    {\n",
    "                                        \"Method\": method,\n",
    "                                        \"CaStLed\": \"CaStLed\" if method.split(\"-\")[0] == \"CaStLe\" else \"Not CaStLed\",\n",
    "                                        \"Replicate ID\": replicate_num,\n",
    "                                        \"Grid Size\": grid_size,\n",
    "                                        \"Time Samples\": T,\n",
    "                                        \"$\\sigma$\": sigma,\n",
    "                                        \"Density\": density,\n",
    "                                        \"$F_1$ Score\": f1_result[0],\n",
    "                                        \"Precision\": f1_result[1],\n",
    "                                        \"Recall\": f1_result[2],\n",
    "                                        \"TP\": f1_result[3],\n",
    "                                        \"FP\": f1_result[4],\n",
    "                                        \"FN\": f1_result[5],\n",
    "                                        \"TN\": f1_result[6],\n",
    "                                    }\n",
    "                                )\n",
    "\n",
    "                        if LOG_DATA_COMPLETED:\n",
    "                            if len(glob.glob(DATA_PATH)) != 30:\n",
    "                                data_incomplete.append((DATA_PATH, len(glob.glob(DATA_PATH))))\n",
    "                            else:\n",
    "                                data_complete.append(RESULTS_PATH)\n",
    "                        if LOG_RESULTS_COMPLETED:\n",
    "                            if len(glob.glob(RESULTS_PATH)) != 30:\n",
    "                                r_incomplete.append((RESULTS_PATH, len(glob.glob(RESULTS_PATH)), grid_size, T, sigma, density))\n",
    "                            else:\n",
    "                                r_complete.append(RESULTS_PATH)\n",
    "                                 \n",
    "        results_df = pd.DataFrame(results_dict_list)\n",
    "        results_df[\"Matthews Correlation Coefficient\"] = results_df.apply(lambda row: matthews_correlation_coefficient(row[\"TP\"], row[\"FP\"], row[\"FN\"], row[\"TN\"]), axis=1)\n",
    "        results_df[\"False Discovery Rate\"] = results_df.apply(lambda row: false_discovery_rate(row[\"FP\"], row[\"TP\"]), axis=1)\n",
    "        results_df[\"True Positive Rate\"] = results_df[\"TP\"] / (results_df[\"TP\"] + results_df[\"FN\"])\n",
    "        results_df[\"False Positive Rate\"] = results_df[\"FP\"] / (results_df[\"FP\"] + results_df[\"TN\"])\n",
    "        results_df[\"False Negative Rate\"] = results_df[\"FN\"] / (results_df[\"TP\"] + results_df[\"FN\"])\n",
    "        results_df[\"True Negative Rate\"] = results_df[\"TN\"] / (results_df[\"FP\"] + results_df[\"TN\"])\n",
    "        \n",
    "        method_meta_data[method].append(results_df)\n",
    "        method_meta_data[method].append((r_incomplete, r_complete, corrupted))\n",
    "\n",
    "        if SAVE_DATA:\n",
    "            if LOG_RESULTS_COMPLETED:\n",
    "                if len(r_incomplete) == 0:\n",
    "                    with open(str(Path.home()) + csv_filename, \"wb\") as f:\n",
    "                        results_df.to_csv(f)\n",
    "            else:\n",
    "                with open(str(Path.home()) + csv_filename, \"wb\") as f:\n",
    "                        results_df.to_csv(f)\n",
    "    idx +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in method_meta_data:\n",
    "    print(m)\n",
    "    try:\n",
    "        print(\"r_incomplete:{}, r_complete:{}, corrupted:{}\".format(len(method_meta_data[m][3][0]), len(method_meta_data[m][3][1]), len(method_meta_data[m][3][2])))\n",
    "        if len(method_meta_data[m][3][0]) > 0:\n",
    "            print([(r[1], r[0]) for r in method_meta_data[m][3][0]])\n",
    "        if len(method_meta_data[m][3][2]) > 0:\n",
    "            print(\"Corrupted files:\")\n",
    "            for f in method_meta_data[m][3][2]:\n",
    "                print(f)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [method_meta_data[method][2] for method in method_meta_data]\n",
    "complete_results = pd.concat(dataframes, ignore_index=True)\n",
    "complete_results[\"Method\"] = complete_results.apply(lambda row: \"Sparse VAR\" if row[\"Method\"] == \"VAR\" else row[\"Method\"], axis=1)\n",
    "complete_results[\"Method\"] = complete_results.apply(lambda row: \"CaStLe-Sparse VAR\" if row[\"Method\"] == \"CaStLe-VAR\" else row[\"Method\"], axis=1)\n",
    "complete_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results[(complete_results[\"Method\"] == \"DYNOTEARS\") & (complete_results[\"Grid Size\"] == 10) & (complete_results[\"Time Samples\"] == 150)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results[(complete_results[\"Method\"] == \"alt-CaStLe-PC\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results[(complete_results[\"Method\"] == \"DYNOTEARS\") & (complete_results[\"Grid Size\"] == 10) & (complete_results[\"Time Samples\"] == 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(len(data_incomplete), len(data_complete))\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results[\"Implementation\"] = complete_results[\"CaStLed\"]\n",
    "complete_results[\"Matthews Correlation Coefficient\"] = complete_results[\"Matthews Correlation Coefficient\"] * 100\n",
    "complete_results[\"$F_1$ Score\"] = complete_results[\"$F_1$ Score\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_results[complete_results[\"Method\"] == \"CaStLe-PC-stable\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context(\"talk\", font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = complete_results[complete_results[\"Density\"] == 0.4]\n",
    "hue_order = [\"CaStLe-Sparse VAR\", \"Sparse VAR\", \"CaStLe-PC\", \"PC\", \"CaStLe-PCMCI\", \"PCMCI\", \"CaStLe-DYNOTEARS\", \"DYNOTEARS\"]\n",
    "style_order = [\"CaStLed\", \"Not CaStLed\"]\n",
    "palette = list(np.repeat(np.array(sns.color_palette(\"bright\")),2,axis=0))\n",
    "g = sns.relplot(data=data[(data[\"Time Samples\"] == 10) | (data[\"Time Samples\"] == 150)], x=\"Grid Size\", y=\"Matthews Correlation Coefficient\",\n",
    "                kind=\"line\", col=\"Time Samples\", hue=\"Method\", hue_order=hue_order, palette=palette, style=\"Implementation\", style_order=style_order, marker=\"o\", aspect=6.5/3.0)\n",
    "g.set_ylabels(\"Matthews\\nCorrelation Coefficient\")\n",
    "\n",
    "handles, labels = g.axes[0][0].get_legend_handles_labels()\n",
    "new_handles = []\n",
    "new_labels = []\n",
    "for idx in range(len(handles)):\n",
    "    if \"Method\" in labels[idx]:\n",
    "        labels[idx] = \"Parent-Identification Phase\"\n",
    "    if \"CaStLed\" in labels[idx]:\n",
    "        new_handles.append(handles[idx])\n",
    "        new_labels.append(labels[idx])\n",
    "    elif \"CaStLe\" not in labels[idx]:\n",
    "        new_handles.append(handles[idx])\n",
    "        new_labels.append(labels[idx])\n",
    "g._legend.remove()\n",
    "g.figure.legend(new_handles, new_labels,bbox_to_anchor=(1.05, 1), frameon=False)\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    xtix = ax.get_xticks()\n",
    "    ax.set_xticklabels([\"{}x{}\".format(int(label), int(label)) for label in xtix])\n",
    "    ax.yaxis.set_major_formatter(StrMethodFormatter(u\"{x:.0f}%\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hue_order = ['Sparse VAR', 'CaStLe-Sparse VAR', 'PC', 'PCMCI', 'CaStLe-PC',\n",
    "                'CaStLe-PC-stable', 'CaStLe-PCMCI', 'DYNOTEARS',\n",
    "                'CaStLe-DYNOTEARS']\n",
    "style_order = [\"CaStLed\", \"Not CaStLed\"]\n",
    "unique_methods = ['CaStLe-PC-stable']\n",
    "grid_sizes = complete_results[\"Grid Size\"].unique()\n",
    "for d in [0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9, 1.0]:\n",
    "    data = complete_results[complete_results[\"Density\"] == d]\n",
    "\n",
    "    # Generate a palette\n",
    "    palette = []\n",
    "    unique_color = sns.color_palette(\"bright\")[-1]  # Unique color for 'CaStLe-PC-stable'\n",
    "    base_color_map = {\n",
    "        'Sparse VAR': sns.color_palette(\"bright\")[0],\n",
    "        'PC': sns.color_palette(\"bright\")[1],\n",
    "        'PCMCI': sns.color_palette(\"bright\")[2],\n",
    "        'DYNOTEARS': sns.color_palette(\"bright\")[3],\n",
    "    }\n",
    "    for method in hue_order:\n",
    "        if method in unique_methods:\n",
    "            palette.append(unique_color)\n",
    "        else:\n",
    "            base_method = method.replace(\"CaStLe-\", \"\").split('-')[0]\n",
    "            if base_method in base_color_map:\n",
    "                palette.append(base_color_map[base_method])\n",
    "            else:\n",
    "                palette.append(sns.color_palette(\"pastel\")[0])\n",
    "\n",
    "    g = sns.relplot(data=data[(data[\"Time Samples\"] == 10) | (data[\"Time Samples\"] == 150)], x=\"Grid Size\", y=\"Matthews Correlation Coefficient\",\n",
    "                    kind=\"line\", col=\"Time Samples\", hue=\"Method\", hue_order=hue_order, palette=palette, style=\"Implementation\", style_order=style_order, marker=\"o\", aspect=6.5/3.0)\n",
    "    g.set_ylabels(\"Matthews\\nCorrelation Coefficient\")\n",
    "\n",
    "    handles, labels = g.axes[0][0].get_legend_handles_labels()\n",
    "    # Adjust labels and handles for the legend\n",
    "    new_handles, new_labels = [], []\n",
    "    for method in hue_order:  # Ensure order and correct naming\n",
    "        for handle, label in zip(handles, labels):\n",
    "            if label == method:  # Match the handle with the method\n",
    "                new_label = label.replace(\"CaStLe-\", \"\")  # Adjust label\n",
    "                if \"CaStLe-PC-stable\" in label:\n",
    "                    new_label = \"PC-Stable-Single\"  # Special case\n",
    "                if new_label not in new_labels:  # Avoid duplicates\n",
    "                    new_handles.append(handle)\n",
    "                    new_labels.append(new_label)\n",
    "\n",
    "    # Add the title for \"Parent-Identification Phase\" and \"Implementation\" manually\n",
    "    new_labels = ['Parent-Identification Phase'] + new_labels\n",
    "    new_handles = [Line2D([0], [0], color='none')] + new_handles  # Placeholder for the title\n",
    "    new_labels += ['Implementation', 'Not CaStLed', 'CaStLed']\n",
    "    style_handles = [handles[labels.index('Not CaStLed')], handles[labels.index('CaStLed')]]\n",
    "    new_handles += [Line2D([0], [0], color='none'), *style_handles]  # Add a placeholder for the title, then the style handles\n",
    "\n",
    "    # Update the legend with the corrected handles and labels\n",
    "    g._legend.remove()\n",
    "    g.figure.legend(handles=new_handles, labels=new_labels, bbox_to_anchor=(1.05, 1), frameon=False, title=None)\n",
    "\n",
    "    for ax in g.axes.flat:\n",
    "        ax.set_xticks(grid_sizes)  # Set the tick locations explicitly\n",
    "        ax.set_xticklabels([f\"{size}x{size}\" for size in grid_sizes])  # Set the tick labels\n",
    "        # Explicitly set the Y-axis ticks to display 0%, 50%, and 100%\n",
    "        ax.set_yticks([0, 50, 100])\n",
    "        ax.yaxis.set_major_formatter(StrMethodFormatter(u\"{x:.0f}%\"))\n",
    "\n",
    "    sparsity_d = int(d*10) if d < 0.5 else int(d*10-1)\n",
    "    plt.suptitle(r\"Sparsity $d={}$\".format(sparsity_d), y=1.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = complete_results.copy()\n",
    "data[\"Density\"] = data.apply(lambda row: int(row[\"Density\"]*10) if row[\"Density\"] < 0.5 else int(row[\"Density\"]*10-1), axis=1)\n",
    "\n",
    "grid_sizes = complete_results[\"Grid Size\"].unique()\n",
    "\n",
    "# Corrected hue_order to include \"CaStLe-PC-stable\"\n",
    "hue_order = [\"CaStLe-Sparse VAR\", \"Sparse VAR\", \"CaStLe-PC\", \"PC\", \"CaStLe-PCMCI\", \"PCMCI\", \"CaStLe-PC-stable\", \"CaStLe-DYNOTEARS\", \"DYNOTEARS\"]\n",
    "style_order = [\"CaStLed\", \"Not CaStLed\"]\n",
    "\n",
    "# Initialize an empty palette list\n",
    "palette = []\n",
    "\n",
    "# Map base methods to colors\n",
    "base_color_map = {\n",
    "    'Sparse VAR': sns.color_palette(\"bright\")[0],\n",
    "    'PC': sns.color_palette(\"bright\")[1],\n",
    "    'PCMCI': sns.color_palette(\"bright\")[2],\n",
    "    'DYNOTEARS': sns.color_palette(\"bright\")[3],\n",
    "}\n",
    "\n",
    "# Unique color for 'CaStLe-PC-stable'\n",
    "unique_color = sns.color_palette(\"bright\")[-1]\n",
    "\n",
    "# Generate the palette\n",
    "for method in hue_order:\n",
    "    if \"CaStLe-PC-stable\" in method:\n",
    "        # Assign the unique color to 'CaStLe-PC-stable'\n",
    "        palette.append(unique_color)\n",
    "    elif \"CaStLe-\" in method:\n",
    "        # For CaStLe- methods, use the corresponding base method color\n",
    "        base_method = method.replace(\"CaStLe-\", \"\").split('-')[0]\n",
    "        palette.append(base_color_map.get(base_method, sns.color_palette(\"pastel\")[0]))\n",
    "    else:\n",
    "        # For non-CaStLe methods, directly use the base method color\n",
    "        palette.append(base_color_map.get(method, sns.color_palette(\"pastel\")[0]))\n",
    "\n",
    "# Now, use this palette in your plot\n",
    "g = sns.relplot(data=data[(data[\"Time Samples\"] == 10) | (data[\"Time Samples\"] == 150)], x=\"Grid Size\", y=\"Matthews Correlation Coefficient\",\n",
    "                kind=\"line\", col=\"Time Samples\", row=\"Density\", hue=\"Method\", hue_order=hue_order, palette=palette, style=\"Implementation\", style_order=style_order, marker=\"o\", aspect=6.5/3.0)\n",
    "g.set_ylabels(\"Matthews\\nCorrelation Coefficient\")\n",
    "\n",
    "for ax in g.axes.flat:\n",
    "    ax.set_xticks(grid_sizes)  # Set the tick locations explicitly\n",
    "    ax.set_xticklabels([f\"{size}x{size}\" for size in grid_sizes])  # Set the tick labels\n",
    "    # Explicitly set the Y-axis ticks to display 0%, 50%, and 100%\n",
    "    ax.set_yticks([0, 50, 100])\n",
    "    ax.yaxis.set_major_formatter(StrMethodFormatter(u\"{x:.0f}%\"))\n",
    "\n",
    "handles, labels = g.axes[0][0].get_legend_handles_labels()\n",
    "new_handles, new_labels = [], []\n",
    "for method in hue_order:  # Ensure order and correct naming\n",
    "    for handle, label in zip(handles, labels):\n",
    "        if label == method:  # Match the handle with the method\n",
    "            new_label = label.replace(\"CaStLe-\", \"\")  # Adjust label for consistency\n",
    "            if \"CaStLe-PC-stable\" in label:\n",
    "                new_label = \"PC-Stable-Single\"  # Special case, rename for clarity\n",
    "            if new_label not in new_labels:  # Avoid duplicates\n",
    "                new_handles.append(handle)\n",
    "                new_labels.append(new_label)\n",
    "\n",
    "# Add the title for \"Parent-Identification Phase\" and \"Implementation\" manually\n",
    "new_labels = ['Parent-Identification Phase'] + new_labels\n",
    "new_handles = [Line2D([0], [0], color='none')] + new_handles  # Placeholder for the title\n",
    "new_labels += ['Implementation', 'Not CaStLed', 'CaStLed']\n",
    "style_handles = [handles[labels.index('Not CaStLed')], handles[labels.index('CaStLed')]]\n",
    "new_handles += [Line2D([0], [0], color='none'), *style_handles]  # Add a placeholder for the title, then the style handles\n",
    "\n",
    "# Update the legend with the corrected handles and labels\n",
    "g._legend.remove()\n",
    "g.figure.legend(handles=new_handles, labels=new_labels, bbox_to_anchor=(1.05, 1), frameon=False, title=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise KeyboardInterrupt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_f1 = plt.figure(figsize=(16,9))\n",
    "g_f1 = sns.catplot(\"Density\", \"Matthews Correlation Coefficient\", \"Time Samples\", col=\"Grid Size\", col_wrap=3, data=results_df, kind=\"box\")\n",
    "g_f1.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_f1.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_grid_medians = results_df.groupby(['Grid Size'])['$F_1$ Score'].median().round(2)\n",
    "vertical_offset = results_df['$F_1$ Score'].median() * 0.02 # offset from median for display\n",
    "\n",
    "g_grids_f1 = sns.boxplot(data=results_df, x=\"Grid Size\", y=\"$F_1$ Score\", color=\"slategrey\")\n",
    "for xtick, median in enumerate(f1_grid_medians):\n",
    "    g_grids_f1.text(xtick, median + vertical_offset, median, \n",
    "            horizontalalignment='center', size='x-small', color='w', weight='semibold')\n",
    "fig = g_grids_f1.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_grid_medians = results_df.groupby(['Grid Size'])[\"Matthews Correlation Coefficient\"].median().round(2)\n",
    "vertical_offset = results_df[\"Matthews Correlation Coefficient\"].median() * 0.05 # offset from median for display\n",
    "\n",
    "g_grids_mcc = sns.boxplot(data=results_df, x=\"Grid Size\", y=\"Matthews Correlation Coefficient\", color=\"slategrey\",)\n",
    "for xtick, median in enumerate(mcc_grid_medians):\n",
    "    g_grids_mcc.text(xtick, median + vertical_offset, median, \n",
    "            horizontalalignment='center', size='x-small', color='w', weight='semibold')\n",
    "fig = g_grids_mcc.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix some params to show fewer datapoints\n",
    "data = results_df[(results_df[\"$\\sigma$\"] == 1.0) & (results_df[\"Density\"] == 0.3) & (results_df[\"Time Samples\"] == 1000)].copy()\n",
    "f1_grid_medians = data.groupby(['Grid Size'])['$F_1$ Score'].median().round(2)\n",
    "f1_vertical_offset = data['$F_1$ Score'].median() * -0.022 # offset from median for display\n",
    "mcc_grid_medians = data.groupby(['Grid Size'])[\"Matthews Correlation Coefficient\"].median().round(2)\n",
    "mcc_vertical_offset = data[\"Matthews Correlation Coefficient\"].median() * -0.022 # offset from median for display\n",
    "\n",
    "fig = plt.figure(figsize=(15,5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "g_grids_f1 = sns.boxplot(data=data, x=\"Grid Size\", y=\"$F_1$ Score\", color=\"slategrey\", notch=True)\n",
    "for xtick, median in enumerate(f1_grid_medians):\n",
    "    g_grids_f1.text(xtick, median + f1_vertical_offset, median, \n",
    "            horizontalalignment='center', size='x-small', color='w', weight='semibold')\n",
    "ax = g_grids_f1.axes\n",
    "new_labels = [\"{}x{}\".format(label.get_text(), label.get_text()) for label in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(new_labels)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "g_grids_mcc = sns.boxplot(data=data, x=\"Grid Size\", y=\"Matthews Correlation Coefficient\", color=\"slategrey\", notch=True)\n",
    "for xtick, median in enumerate(mcc_grid_medians):\n",
    "    g_grids_mcc.text(xtick, median + mcc_vertical_offset, median, \n",
    "            horizontalalignment='center', size='x-small', color='w', weight='semibold')\n",
    "ax = g_grids_mcc.axes\n",
    "new_labels = [\"{}x{}\".format(label.get_text(), label.get_text()) for label in ax.get_xticklabels()]\n",
    "ax.set_xticklabels(new_labels)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "# Fix some params to show fewer datapoints\n",
    "data = results_df[(results_df[\"$\\sigma$\"] == 1.0) & (results_df[\"Density\"] == 0.3) & (results_df[\"Grid Size\"] == 10)].copy()\n",
    "mcc_t_medians = data.groupby(['Time Samples'])[\"Matthews Correlation Coefficient\"].median().round(2)\n",
    "vertical_offset = data[\"Matthews Correlation Coefficient\"].median() * 0.01 # offset from median for display\n",
    "\n",
    "g_t_mcc = sns.boxplot(data=data, x=\"Time Samples\", y=\"Matthews Correlation Coefficient\", color=\"slategrey\", notch=True, )\n",
    "for xtick, median in enumerate(mcc_t_medians):\n",
    "    g_t_mcc.text(xtick, median + vertical_offset, median, \n",
    "            horizontalalignment='center', size='x-small', color='w', weight='semibold', fontsize=12)\n",
    "g_t_mcc.tick_params(axis=\"x\", labelrotation=45)\n",
    "fig = g_t_mcc.get_figure() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mcc_sigma_medians = results_df.groupby(['$\\sigma$'])[\"Matthews Correlation Coefficient\"].median().round(2)\n",
    "vertical_offset = results_df[\"Matthews Correlation Coefficient\"].median() * 0.03 # offset from median for display\n",
    "\n",
    "g_sigma_mcc = sns.boxplot(data=results_df, x=\"$\\sigma$\", y=\"Matthews Correlation Coefficient\", color=\"slategrey\", notch=False)\n",
    "for xtick, median in enumerate(mcc_sigma_medians):\n",
    "    g_sigma_mcc.text(xtick, median + vertical_offset, median, \n",
    "            horizontalalignment='center', size='x-small', color='w', weight='semibold')\n",
    "plt.xlabel(\"Sigma (St. Dev. of added Gaussian noise per timestep)\")\n",
    "plt.ylabel(\"Matthews Correlation Coefficient\");\n",
    "fig = g_sigma_mcc.get_figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "g_f1 = sns.FacetGrid(results_df, col=\"Time Samples\", row=\"Grid Size\", )\n",
    "g_f1.map(sns.boxplot, \"Density\", \"$F_1$ Score\", order=DENSITIES, );\n",
    "g_f1.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_f1.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_f1.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mcc = sns.FacetGrid(results_df, col=\"Time Samples\", row=\"Grid Size\")\n",
    "g_mcc.map(sns.boxplot, \"Density\", \"Matthews Correlation Coefficient\", order=DENSITIES);\n",
    "g_mcc.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_mcc.set_ylabels(\"MCC\")\n",
    "g_mcc.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_mcc.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fdr = sns.FacetGrid(results_df[results_df[\"Time Samples\"] == 10], col=\"Time Samples\", row=\"Grid Size\")\n",
    "g_fdr.map(sns.boxplot, \"Density\", \"False Discovery Rate\", order=DENSITIES,);\n",
    "g_fdr.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_fdr.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_fdr.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tpr = sns.FacetGrid(results_df, col=\"Time Samples\", row=\"Grid Size\")\n",
    "g_tpr.map(sns.boxplot, \"Density\", \"True Positive Rate\", order=DENSITIES,);\n",
    "g_tpr.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_tpr.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_tpr.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fpr = sns.FacetGrid(results_df, col=\"Time Samples\", row=\"Grid Size\")\n",
    "g_fpr.map(sns.boxplot, \"Density\", \"False Positive Rate\", order=DENSITIES,);\n",
    "g_fpr.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_fpr.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_fpr.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_tnr = sns.FacetGrid(results_df, col=\"Time Samples\", row=\"Grid Size\")\n",
    "g_tnr.map(sns.boxplot, \"Density\", \"True Negative Rate\", order=DENSITIES,);\n",
    "g_tnr.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_tnr.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_tnr.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_fnr = sns.FacetGrid(results_df, col=\"Time Samples\", row=\"Grid Size\")\n",
    "g_fnr.map(sns.boxplot, \"Density\", \"False Negative Rate\", order=DENSITIES,);\n",
    "g_fnr.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "g_fnr.set_xticklabels(DENSITY_LABELS)\n",
    "for ax in g_fnr.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True, labelrotation=0)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mcc_lines = sns.FacetGrid(results_df[results_df[\"Time Samples\"].isin([150, 350, 575, 775, 1000])], col=\"Time Samples\", row=\"Grid Size\", hue=\"Density\", legend_out=True, palette=\"magma_r\")\n",
    "g_mcc_lines.map(sns.lineplot, \"$\\sigma$\", \"Matthews Correlation Coefficient\", marker='o');\n",
    "g_mcc_lines.set_titles(\"{row_var}={row_name}\\n{col_var}={col_name}\")\n",
    "for ax in g_mcc_lines.axes.flat:\n",
    "    ax.tick_params(axis=\"x\", labelbottom=True)\n",
    "g_mcc_lines.set_ylabels(\"MCC\")\n",
    "plt.tight_layout()\n",
    "g_mcc_lines.add_legend(labels=DENSITY_LABELS)\n",
    "sns.move_legend(g_mcc_lines, \"lower center\", ncol=10, bbox_to_anchor=(0.5, -0.05), frameon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_medians = results_df.groupby(['$\\sigma$'])[\"Matthews Correlation Coefficient\"].median().round(2)\n",
    "test_medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_df = results_df[results_df[\"$\\sigma$\"] ==  0.5].copy()\n",
    "filtered_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_mcc_density = sns.FacetGrid(filtered_results_df, hue=\"Grid Size\", col=\"Density\", col_wrap=3, palette=\"magma_r\")\n",
    "g_mcc_density.map(sns.lineplot, \"Time Samples\", \"Matthews Correlation Coefficient\", markers=True, ci=None);\n",
    "g_mcc_density.add_legend()\n",
    "g_mcc_density.set_ylabels(\"MCC\")\n",
    "for ax, density in zip(g_mcc_density.axes.flat, DENSITY_LABELS):\n",
    "    ax.tick_params(labelbottom=True)\n",
    "    ax.set_title(\"Density = \" + density)\n",
    "sns.move_legend(g_mcc_density, \"lower center\", ncol=10, bbox_to_anchor=(0.5, -0.1), frameon=True)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_results_df[\"T/grid\"] = filtered_results_df.apply(lambda row: (row[\"Time Samples\"] / row[\"Grid Size\"]), axis=1)\n",
    "\n",
    "g_test = sns.FacetGrid(filtered_results_df, hue=\"Time Samples\", col=\"Density\", col_wrap=5)\n",
    "g_test.map(sns.lineplot, \"T/grid\", \"Matthews Correlation Coefficient\", markers=True, ci=None);\n",
    "g_test.add_legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting = 0.5\n",
    "step = 0.1\n",
    "ending = 0.9\n",
    "\n",
    "prob_of_success = pd.DataFrame()\n",
    "prob_of_success = filtered_results_df.groupby([\"Grid Size\", \"Time Samples\", \"Density\"])[\"Matthews Correlation Coefficient\"].apply(lambda c: (c >= starting).sum()/len(c))\n",
    "prob_of_success = prob_of_success.reset_index().rename(columns={\"Matthews Correlation Coefficient\": \"Probability of Exceeding Threshold\"})\n",
    "prob_of_success[\"MCC Threshold\"] = starting\n",
    "for threshold in [round(i, 1) for i in np.arange(starting + step, ending, step)]:\n",
    "    new_frame = pd.DataFrame()\n",
    "    new_frame = filtered_results_df.groupby([\"Grid Size\", \"Time Samples\", \"Density\"])[\"Matthews Correlation Coefficient\"].apply(lambda c: (c >= threshold).sum()/len(c))\n",
    "    new_frame = new_frame.reset_index().rename(columns={\"Matthews Correlation Coefficient\": \"Probability of Exceeding Threshold\"})\n",
    "    new_frame[\"MCC Threshold\"] = threshold\n",
    "    prob_of_success = pd.concat(\n",
    "        [\n",
    "            prob_of_success,\n",
    "            new_frame\n",
    "        ], ignore_index=True,\n",
    "    )\n",
    "prob_of_success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g_prob_of_success = sns.lmplot(data=prob_of_success[prob_of_success[\"Density\"] == 0.6], x=\"Grid Size\", y=\"Probability of Exceeding Threshold\", col=\"MCC Threshold\", hue=\"Time Samples\", col_wrap=2, palette=\"magma_r\",)\n",
    "    for ax in g_prob_of_success.axes.flat:\n",
    "        ax.tick_params(axis=\"x\", labelbottom=True)\n",
    "        ax.set(ylim=(-0.1, 1.1))\n",
    "    sns.move_legend(g_prob_of_success, \"upper left\", bbox_to_anchor=(.62, .28), frameon=True, ncol=2)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cldera_causal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f748049fe468f6160372f53eb3b9abf8f67851a7dbd8d700357ac2c3d411157c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
